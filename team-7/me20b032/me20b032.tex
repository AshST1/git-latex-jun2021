\section{Archish S me20b032}
A Markov decision process can be described as a tuple $ \langle S, A, T, R \rangle $, where
\begin{itemize}
	\item $S$ is a finite set of states of the world;
	\item $A$ is a finite set of actions;
	\item $ T : S \times A \rightarrow \Pi(S)$ is the \textit{state-transition function}, giving for each world state and agent action, a probability distribution over world states (we write $T(s,a,s^\prime)$ for the probability of ending in state $s^\prime$, gievn that the agent starts in state $s$ and takes action $a$);
	\item $ R : S \times A \rightarrow \mathbb{R}$ is the reward function, giving the expected immediate reward gained by the agent for taking each action in each state (we write $R(s,a)$ for the expected reward for taking action $a$ in state $s$);
	\item A stationary policy, $ \pi : S \rightarrow A $, is a situation-action mapping that specifies, for each state, an action to be taken.
\end{itemize}

\begin{equation}
	V_{\pi}(s) = R(s,\pi(s)) + \gamma \sum_{s^\prime \in S} T(s, \pi(s), s^\prime) V_{\pi}(s^\prime)
\end{equation}